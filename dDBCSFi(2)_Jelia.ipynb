{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75X9dbSdDI_s"
      },
      "source": [
        "# Code used for the experiments of \"*Opening Up the Neural Network Classifier for Shap Score Computation*\", submitted to Jelia 2023\n",
        "\n",
        "(It should be executed on descending order)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e58yPuk2k20"
      },
      "source": [
        "# BNN and dDBCSFi(2) creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtjZE5zd3e3H"
      },
      "source": [
        "## Random seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qrl05Et-2BRz"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import os\n",
        "from numpy.random import seed\n",
        "import random as python_random\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "def set_random_seeds(py_hash_seed=\"7\", np_seed=7, rand_seed=7, tf_seed=7):\n",
        "  os.environ['PYTHONHASHSEED'] = py_hash_seed\n",
        "  seed(np_seed)\n",
        "  python_random.seed(rand_seed)\n",
        "  set_seed(tf_seed)\n",
        "  \n",
        "set_random_seeds()\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK9TuJLY3YQk"
      },
      "source": [
        "## Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgTAgN-P2qAM"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import time\n",
        "\n",
        "def seconds_separator(seconds_passed):\n",
        "  duration = seconds_passed\n",
        "  hours = int(duration//3600)\n",
        "  duration = duration%3600\n",
        "  minutes = int(duration//60)\n",
        "  duration = float(round(duration%60, 4))\n",
        "  return str(hours)+\":\"+\"0\"*(minutes<10)+str(minutes)+\":\"+\"0\"*(duration<10)+str(duration)+\"0\"*(6-len(str(duration))+(duration>=10))\n",
        "\n",
        "if (not os.path.exists(\"riss-solver\")):\n",
        "  beginning = time.monotonic()\n",
        "  #Installation of libraries to work with SSDs\n",
        "  !git clone https://github.com/Jorvan758/nnf2sdd.git\n",
        "  %cd ./nnf2sdd\n",
        "  %mv * ../\n",
        "  %cd ../\n",
        "  !rm -rf ./nnf2sdd\n",
        "  !pip install pyeda pysdd graphviz\n",
        "  #Installation of the simplifying solver\n",
        "  !git clone https://github.com/Jorvan758/CNF_OBDD.git\n",
        "  %cd CNF_OBDD/bnn/BinaryNet/cpp/\n",
        "  %mv * ../../../../\n",
        "  %cd ../../../../\n",
        "  !rm -r CNF_OBDD\n",
        "  %cd riss-solver/\n",
        "  !sudo apt-get install zlib1g-dev\n",
        "  !sudo apt-get install curl\n",
        "  !cmake -D CMAKE_BUILD_TYPE=Release\n",
        "  !make\n",
        "  %cd ../\n",
        "  #Larq\n",
        "  !pip install larq\n",
        "  clear_output()\n",
        "  duration = time.monotonic() - beginning\n",
        "  print(\"Time taken in installations:\", seconds_separator(duration))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuYCpLzd3Z5v"
      },
      "source": [
        "## Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxKhRjR_3QqQ"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "\n",
        "from tensorflow.keras import *\n",
        "from tensorflow import nn, matmul\n",
        "from tensorflow import round as kround\n",
        "from tensorflow.keras.constraints import MinMaxNorm\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "import larq as lq\n",
        "import networkx as nx\n",
        "from networkx.drawing.nx_agraph import graphviz_layout\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def step_tanh(x):\n",
        "  return kround(nn.sigmoid(x))*2 - 1\n",
        "def step_sigm(x):\n",
        "  return kround(nn.sigmoid(x))\n",
        "\n",
        "def plot_the_network(the_model, resplt=10):\n",
        "  the_weights = {}\n",
        "  G = nx.Graph()\n",
        "  the_colors = []\n",
        "  previous_layer = []\n",
        "  for node in range(the_model.input_shape[1]):\n",
        "    G.add_node(str(node+1), layer=0)\n",
        "    previous_layer += [str(node+1)]\n",
        "    the_colors += [\"b\"]\n",
        "  G.add_node(\"_\"+str(0), layer=0)\n",
        "  previous_layer += [\"_\"+str(0)]\n",
        "  the_colors += [\"g\"]\n",
        "  for the_layer in range(len(the_model.get_weights())//2):\n",
        "    new_layer = []\n",
        "    for node in range(the_model.get_weights()[the_layer*2].shape[1]):\n",
        "      G.add_node(str(the_layer+1)+\"_\"+str(node+1), layer=the_layer+1)\n",
        "      new_layer += [str(the_layer+1)+\"_\"+str(node+1)]\n",
        "      the_colors += [\"r\"]\n",
        "    index2 = 0\n",
        "    for node_o in previous_layer:\n",
        "      index1 = 0\n",
        "      for node_d in new_layer:\n",
        "        G.add_edge(node_o, node_d)\n",
        "        if (\"_0\" in node_o): the_weights[(node_o, node_d)] = round(the_model.get_weights()[the_layer*2+1][index1], 2)\n",
        "        else: the_weights[(node_o, node_d)] = round(the_model.get_weights()[the_layer*2][index2][index1], 2)\n",
        "        index1 += 1\n",
        "      index2 += 1\n",
        "    if (the_layer < (len(the_model.get_weights())//2-1)):\n",
        "      G.add_node(str(the_layer+1)+\"_\"+str(0), layer=the_layer+1)\n",
        "      new_layer += [str(the_layer+1)+\"_\"+str(0)]\n",
        "      the_colors += [\"g\"]\n",
        "    previous_layer = new_layer\n",
        "  pos = nx.multipartite_layout(G, subset_key=\"layer\")\n",
        "  plt.clf()\n",
        "  plt.figure(figsize=[resplt,resplt])\n",
        "  nx.draw(G, pos, edge_color='black', width=2, linewidths=1, node_size=100*resplt/(len(the_model.get_weights())//2),\n",
        "          node_color=the_colors, alpha=0.9, with_labels=False)\n",
        "  nx.draw_networkx_edge_labels(G, pos, edge_labels=the_weights, label_pos=0.2, rotate=False,\n",
        "                               font_size=15*resplt/((len(the_model.get_weights())//2)+resplt))\n",
        "  nx.draw(G, pos, edge_color='black', width=2, edgelist=[], linewidths=1, node_size=100*resplt/(len(the_model.get_weights())//2),\n",
        "          node_color=the_colors, alpha=0.9, with_labels=False)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2zuG8d-6yYT"
      },
      "source": [
        "## CNF formula"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#These are the times taken for a generic BNN with 2 layers for a different number of variables:\n",
        "#2:  0:00:00.0043\n",
        "#3:  0:00:00.0121\n",
        "#4:  0:00:00.0264\n",
        "#5:  0:00:00.0492\n",
        "#6:  0:00:00.0965\n",
        "#7:  0:00:00.2507\n",
        "#8:  0:00:00.6960\n",
        "#9:  0:00:03.1606\n",
        "#10: 0:00:17.4078\n",
        "#11: 0:01:59.9104\n",
        "#12: 0:10:33.7824\n",
        "#13: 1:33:54.6341\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "def create_array(values:int=1, n_variables:int=1) -> np.array:\n",
        "  return np.array([0]*(abs(values)-1)+[(values>0)*2-1]+[0]*(n_variables-abs(values)), dtype=np.int8).reshape(1,-1)\n",
        "\n",
        "def delete_zeros(matrix:np.array, n_variables:int) -> np.array:\n",
        "  return matrix[np.where((matrix == 0).sum(axis=1) != n_variables)[0]]\n",
        "\n",
        "def simplify_cnf_formula(matrix:np.array, n_variables:int) -> np.array:\n",
        "  simplified = matrix.copy()\n",
        "  for x in range(n_variables-1, 0, -1):\n",
        "    with_x_zeros = np.where((simplified == 0).sum(axis=1) == x)[0]\n",
        "    if (with_x_zeros.size != 0) and ((simplified == 0).sum(axis=1).min() < x):\n",
        "      comparables = simplified[with_x_zeros]\n",
        "      erasables = simplified[np.where((simplified == 0).sum(axis=1) < x)[0]]\n",
        "      with_more_zeros = np.where((simplified == 0).sum(axis=1) > x)[0]\n",
        "      if (with_more_zeros.size != 0):\n",
        "        saveables = simplified[with_more_zeros]\n",
        "        for clause in comparables: erasables = erasables[np.where((erasables*(clause!=0) != clause).sum(axis=1) > 0)[0]]\n",
        "        simplified = np.vstack([saveables, comparables, erasables])\n",
        "      else:\n",
        "        for clause in comparables: erasables = erasables[np.where((erasables*(clause!=0) != clause).sum(axis=1) > 0)[0]]\n",
        "        simplified = np.vstack([comparables, erasables])\n",
        "  return simplified\n",
        "\n",
        "def conjunction_cnfs(matrix1:np.array, matrix2:np.array, n_variables:int) -> np.array:\n",
        "  return simplify_cnf_formula(np.unique(np.vstack([matrix1, matrix2]), axis=0), n_variables)\n",
        "\n",
        "def disjunction_cnfs(matrix1:np.array, matrix2:np.array, n_variables:int) -> np.array:\n",
        "  new_ones = []\n",
        "  for clause in matrix1:\n",
        "    new_ones += [delete_zeros((matrix2+clause-clause*(matrix2==clause))*((matrix2*clause < 0).sum(axis=1) == 0).reshape(-1,1), n_variables)]\n",
        "  return simplify_cnf_formula(np.unique(np.vstack(new_ones), axis=0), n_variables)\n",
        "\n",
        "def cnf_negation(matrix:np.array, n_variables:int) -> np.array:\n",
        "  final = np.zeros((1, n_variables))\n",
        "  clauses = -matrix\n",
        "  while (clauses.shape[0] > 0):\n",
        "    new_ones = []\n",
        "    for ind_clause2 in range(n_variables):\n",
        "      if (clauses[0,ind_clause2] != 0):\n",
        "        temp = create_array(clauses[0,ind_clause2]*(ind_clause2+1), n_variables)\n",
        "        new_ones += [delete_zeros((final+temp-temp*(final==temp))*((final*temp < 0).sum(axis=1) == 0).reshape(-1,1), n_variables)]\n",
        "    final = np.unique(np.vstack(new_ones), axis=0)\n",
        "    clauses = clauses[1:]\n",
        "  return simplify_cnf_formula(final, n_variables)\n",
        "\n",
        "def encode_network(the_model, input_file=\"cnf_formula.cnf\") -> str:\n",
        "  beginning = time.monotonic()\n",
        "  n_inputs = the_model.input_shape[1]\n",
        "  inputs = [create_array(i, n_inputs) for i in range(1, n_inputs+1)]\n",
        "  n_layer = 1\n",
        "  for layer in the_model.layers[1:]:\n",
        "    the_weights = layer.get_weights()\n",
        "    outputs = []\n",
        "    for id_neuron in range(layer.output_shape[1]):\n",
        "      print(f'{seconds_separator(time.monotonic() - beginning)}   Layer: {n_layer}/{len(the_model.layers)-1} | Neuron: {id_neuron+1}/{layer.output_shape[1]}')\n",
        "      D = ceil((-the_weights[1][id_neuron] + the_weights[0][:,id_neuron].sum())/2) + (the_weights[0][:,id_neuron] == -1).sum()\n",
        "      previous = {}\n",
        "      for id_input in range(len(inputs)):\n",
        "        if (layer == the_model.layers[-1]): print(f'{seconds_separator(time.monotonic() - beginning)}   Working with the first {id_input+1} inputs')\n",
        "        actual = {}\n",
        "        if (the_weights[0][id_input,id_neuron] == 1): x = inputs[id_input]\n",
        "        else: x = cnf_negation(inputs[id_input], len(inputs))\n",
        "        for d in range(D):\n",
        "          if (id_input < d): break\n",
        "          if (len(inputs) < id_input+1+D-(d+1)): continue\n",
        "          if (d == 0):\n",
        "            if (id_input == 0): actual[d] = x\n",
        "            else: actual[d] = disjunction_cnfs(x, previous[d], len(inputs))\n",
        "          elif (id_input == d): actual[d] = conjunction_cnfs(x, previous[d-1], len(inputs))\n",
        "          else:\n",
        "            temp = conjunction_cnfs(x, previous[d-1], len(inputs))\n",
        "            actual[d] = disjunction_cnfs(temp, previous[d], len(inputs))\n",
        "        previous = actual\n",
        "      outputs += [previous[D-1].astype(dtype=np.int8)]\n",
        "    inputs = outputs\n",
        "    n_layer += 1\n",
        "  print(f'Total time taken: {seconds_separator(time.monotonic() - beginning)}')\n",
        "  dimacs_cnf = inputs[-1]\n",
        "  dimacs_cnf = str(dimacs_cnf*np.arange(1,dimacs_cnf.shape[1]+1)).replace(\" 0\", \"\").replace(\"]\",\"\").replace(\"[\",\"\").replace(\"\\n\", \" 0\\n\")+\" 0\\n\"\n",
        "  while \"  \" in dimacs_cnf: dimacs_cnf = dimacs_cnf.replace(\"  \", \" \")\n",
        "  output_file = input_file[:-4]+\"_final.cnf\"\n",
        "  with open(input_file, 'w') as f:\n",
        "    f.write('p cnf %d %d\\n' % (n_inputs, dimacs_cnf.count('\\n')))\n",
        "    f.write(dimacs_cnf)\n",
        "  return input_file\n",
        "  with open(\"whitelist.txt\", 'w+') as f:\n",
        "    f.write(str(1)+\"..\"+str(n_inputs))\n",
        "  !riss-solver/bin/coprocessor -dimacs=$output_file -verb=0 -config=\"-enabled_cp3 -cp3_stats -bve -bve_red_lits=1 -fm -no-cp3_limited -unhide -cp3_uhdIters=5 -cp3_uhdEE -cp3_uhdTrans -bce -bce-cle -no-bce-bce -dense -xor -no-xorFindSubs -xorEncSize=3 -xorLimit=100000 -no-xorKeepUsed -cp3_iters=2 -ee -cp3_ee_level=3 -cp3_ee_it -rlevel=2 -bve_early -revMin -init-act=3 -actStart=2048 -keepWorst=0.01 -whiteList=whitelist.txt\" $input_file > /dev/null\n",
        "  with open(input_file, \"r\") as file:\n",
        "    c_o = int(file.readline()[:-1].split(\" \")[3])\n",
        "  with open(output_file, \"r\") as file:\n",
        "    c_n = int(file.readline()[:-1].split(\" \")[3])\n",
        "  if (c_o < c_n): return input_file\n",
        "  else: return output_file"
      ],
      "metadata": {
        "id": "YR04IGHNN567"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g370GikG3buz"
      },
      "source": [
        "## SDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aHN2yHL3pOb"
      },
      "outputs": [],
      "source": [
        "from circuits.linear import Classifier\n",
        "from pysdd.sdd import SddManager\n",
        "from graphviz import Source\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def plot_the_sdd(node, n_variables):\n",
        "  result = node.dot()\n",
        "  if (n_variables < 9):\n",
        "    SUB = {0: \"\\N{SUBSCRIPT ZERO}\",\n",
        "          1: \"\\N{SUBSCRIPT ONE}\",\n",
        "          2: \"\\N{SUBSCRIPT TWO}\",\n",
        "          3: \"\\N{SUBSCRIPT THREE}\",\n",
        "          4: \"\\N{SUBSCRIPT FOUR}\",\n",
        "          5: \"\\N{SUBSCRIPT FIVE}\",\n",
        "          6: \"\\N{SUBSCRIPT SIX}\",\n",
        "          7: \"\\N{SUBSCRIPT SEVEN}\",\n",
        "          8: \"\\N{SUBSCRIPT EIGHT}\",\n",
        "          9: \"\\N{SUBSCRIPT NINE}\"}\n",
        "    for i in range(1, n_variables+1): result = result.replace(chr(i + 64)+\"\\\"\",\"x\"+SUB[i]+\" \\\"\").\\\n",
        "                                                      replace(chr(i + 64)+\"|\",\"x\"+SUB[i]+\" |\").replace(\"&not;&not;\", \"\")\n",
        "  return Source(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udShaH-e3ong"
      },
      "source": [
        "## dDBCSFi(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxlRdqIE4aaU"
      },
      "outputs": [],
      "source": [
        "from math import factorial\n",
        "import pickle\n",
        "\n",
        "class InvalidInputs4dDBCSFi_2(Exception):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"You need to pass a SDD or a binary perceptron.\")\n",
        "        \n",
        "class InvalidInputs4SHAP(Exception):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"You need to pass vectors with the correct number of variables.\")\n",
        "\n",
        "class dDBCSFi_2:\n",
        "  def __init__(self, n_variables, SDD=None, perceptron=None):\n",
        "    if (SDD == None) and (perceptron == None): raise InvalidInputs4dDBCSFi_2()\n",
        "    self.SDD = SDD\n",
        "    self.perceptron = perceptron\n",
        "    self.formula = []\n",
        "    self.variables = []\n",
        "    self.n_vars = n_variables\n",
        "    self.vars = np.arange(1, n_variables+1, dtype=np.int8)\n",
        "    self.probabilities = [0.5]*n_variables\n",
        "\n",
        "  def __compile_bnn_step(self, disjunction):\n",
        "    formula = []\n",
        "    set_o = set()\n",
        "    #Iterates over all elements of the disjunction\n",
        "    for conjunction in disjunction.elements():\n",
        "      set_a = set()\n",
        "      elements = []\n",
        "      #Iterate over all the elements of the conjunction\n",
        "      for element in conjunction:\n",
        "        if (element.is_literal()): #Element\n",
        "          ide = str(element)[13:str(element).index(\",\")]\n",
        "          if (ide[:1] == \"-\"):\n",
        "            set_a.add(self.vars[int(ide[1:])-1])\n",
        "            ide = -self.vars[int(ide[1:])-1]\n",
        "          else:\n",
        "            set_a.add(self.vars[int(ide)-1])\n",
        "            ide = self.vars[int(ide)-1]\n",
        "        elif (element.is_decision()): #Formula\n",
        "          ide = self.__compile_bnn_step(element)\n",
        "          for id in ide[1]: set_a.add(self.vars[id-1])\n",
        "          ide = ide[0]\n",
        "          if (ide == []): ide = \"0\"\n",
        "        elif (element.is_true()): ide = \"1\" #True\n",
        "        else: ide = \"0\" #False\n",
        "        if (ide != \"1\"):\n",
        "          elements += [ide]\n",
        "          if (ide == \"0\"): break\n",
        "      #Check that the conjunction is not always F\n",
        "      if (\"0\" in elements): continue\n",
        "      else:\n",
        "        #Check if the conjunction is always T\n",
        "        if (elements == []):\n",
        "          formula = \"1\"\n",
        "          set_o = set()\n",
        "          break\n",
        "        else: #The conjunction varies\n",
        "          if (not isinstance(formula, list)) or (formula != []): #(When the formula is a negative number, it seems to be confusing\n",
        "                                                                 #it with a list, so I had to add a condition to see if it's a list)\n",
        "            for id in set_o: #Check that the new formula matches the old\n",
        "              if (id not in set_a):\n",
        "                if (len(elements) == 1): elements = [False, elements[0], [True, self.vars[id-1], -self.vars[id-1]]]\n",
        "                elif (len(elements) == 2): elements = [False, [False]+elements, [True, self.vars[id-1], -self.vars[id-1]]]\n",
        "                else: elements = [False, elements, [True, self.vars[id-1], -self.vars[id-1]]]\n",
        "            for id in set_a: #Check that the old formula matches the new\n",
        "              if (id not in set_o):\n",
        "                if (formula == []): formula = [True, self.vars[id-1], -self.vars[id-1]]\n",
        "                else: formula = [False, formula, [True, self.vars[id-1], -self.vars[id-1]]]\n",
        "                set_o.add(self.vars[id-1])\n",
        "            #Combine the formulas\n",
        "            if (len(elements) == 1): formula = [True, formula, elements[0]]\n",
        "            elif (len(elements) == 2): formula = [True, formula, [False]+elements]\n",
        "            else: formula = [True, formula, elements]\n",
        "          else: #No need to smooth\n",
        "            for id in set_a: set_o.add(self.vars[id-1])\n",
        "            if (len(elements) == 1): formula = elements[0]\n",
        "            else: formula = [False]+elements\n",
        "    return formula, set_o\n",
        "\n",
        "  def __print_formula_step(self, formula, depth=0):\n",
        "    if (formula[0]): print(\" \"*depth+\"Or\")\n",
        "    else: print(\" \"*depth+\"And\")\n",
        "    if isinstance(formula[1], list):\n",
        "      self.__print_formula_step(formula[1], depth+1)\n",
        "    else:\n",
        "      print(\" \"*(depth+1)+str(formula[1]))\n",
        "    if isinstance(formula[2], list):\n",
        "      self.__print_formula_step(formula[2], depth+1)\n",
        "    else:\n",
        "      print(\" \"*(depth+1)+str(formula[2]))\n",
        "\n",
        "  def __evaluate_formula_step(self, formula, assignment=[]):\n",
        "    #Check the first element\n",
        "    if isinstance(formula[1], list): value1 = self.__evaluate_formula_step(formula[1], assignment)\n",
        "    else: value1 = (assignment[abs(formula[1])-1] == np.sign(formula[1]))\n",
        "    #Check the second element\n",
        "    if isinstance(formula[2], list): value2 = self.__evaluate_formula_step(formula[2], assignment)\n",
        "    else: value2 = (assignment[abs(formula[2])-1] == np.sign(formula[2]))\n",
        "    #Evaluate the formula (I think they can be combined, but for now I don't need to and it's easier to read that way)\n",
        "    if (formula[0]): #It's a disjunction\n",
        "      if (value1) or (value2): finalvalue = True\n",
        "      else: finalvalue = False\n",
        "    else: #It's a conjunction\n",
        "      if (value1) and (value2): finalvalue = True\n",
        "      else: finalvalue = False\n",
        "    return finalvalue\n",
        "\n",
        "  def __count_nodes_step(self, formula):\n",
        "    #Check the first element\n",
        "    if isinstance(formula[1], list): value1 = self.__count_nodes_step(formula[1])\n",
        "    else: value1 = 1\n",
        "    #Check the second element\n",
        "    if isinstance(formula[2], list): value2 = self.__count_nodes_step(formula[2])\n",
        "    else: value2 = 1\n",
        "    return value1+value2+1\n",
        "  \n",
        "  def __encode_perceptron(self):\n",
        "    the_weights = self.perceptron.layers[-1].get_weights()[0].reshape(-1)\n",
        "    the_bias = self.perceptron.layers[-1].get_weights()[1].reshape(-1)\n",
        "    D = ceil((-the_bias + the_weights.sum())/2) + (the_weights == -1).sum()\n",
        "    previous = {}\n",
        "    smoothness = []\n",
        "    for id_input in range(self.n_vars):\n",
        "      actual = {}\n",
        "      if (the_weights[id_input] == 1): x = self.vars[id_input]\n",
        "      else: x = -self.vars[id_input]\n",
        "      for d in range(D):\n",
        "        if (id_input < d): break\n",
        "        if (self.n_vars < id_input+1+D-(d+1)): continue\n",
        "        if (d == 0):\n",
        "          if (id_input == 0): actual[d] = x\n",
        "          else: actual[d] = [True, [False, x, smoothness], [False, -x, previous[d]]]\n",
        "        elif (id_input == d): actual[d] = [False, x, previous[d-1]]\n",
        "        else: actual[d] = [True, [False, x, previous[d-1]], [False, -x, previous[d]]]\n",
        "      previous = actual\n",
        "      if (smoothness == []): smoothness = [True, self.vars[id_input], -self.vars[id_input]]\n",
        "      else: smoothness = [False, smoothness, [True, self.vars[id_input], -self.vars[id_input]]]\n",
        "    return previous[D-1], set(self.vars.flatten())\n",
        "\n",
        "  def compile_bnn(self):\n",
        "    if (self.SDD == None):\n",
        "      self.formula, self.variables = self.__encode_perceptron()\n",
        "    else:\n",
        "      if (self.SDD.is_false()): self.formula, self.variables = \"1\", set()\n",
        "      elif (self.SDD.is_true()): self.formula, self.variables = [], set()\n",
        "      elif (self.SDD.is_decision()):\n",
        "        self.formula, self.variables = self.__compile_bnn_step(self.SDD)\n",
        "        if (not isinstance(self.formula, list)) and (self.formula != \"1\"): self.formula = [self.formula]\n",
        "      else:\n",
        "        ide = str(self.SDD)[13:str(self.SDD).index(\",\")]\n",
        "        if (ide[:1] == \"-\"): self.formula, self.variables = [self.vars[int(ide[1:])-1]], set([self.vars[int(ide[1:])-1]])\n",
        "        else: self.formula, self.variables = [-self.vars[int(ide)-1]], set([self.vars[int(ide)-1]])\n",
        "\n",
        "  def print_formula(self, profundidad=0):\n",
        "    if (self.formula == \"1\"): print(\"It's always True\")\n",
        "    elif (self.formula == []): print(\"It's always False\")\n",
        "    elif (len(self.formula) != 1): self.__print_formula_step(self.formula, profundidad)\n",
        "    else: print(self.formula[0])\n",
        "\n",
        "  def evaluate_formula(self, assignment=[]):\n",
        "    if (self.formula == \"1\"): return 1.0\n",
        "    elif (self.formula == []): return 0.0\n",
        "    elif (len(self.formula) != 1): return float(self.__evaluate_formula_step(self.formula, assignment))\n",
        "    else: return float(assignment[abs(self.formula[0])-1] == np.sign(self.formula[0]))\n",
        "  \n",
        "  def evaluate_formulas(self, assignments=[]):\n",
        "    results = []\n",
        "    for assignment in np.array(assignments):\n",
        "      results += [self.evaluate_formula(assignment)]\n",
        "    return results\n",
        "\n",
        "  def count_nodes(self):\n",
        "    if (self.formula == \"1\") or (self.formula == []): return 1\n",
        "    elif (len(self.formula) == 1): return 1\n",
        "    else: return self.__count_nodes_step(self.formula)\n",
        "\n",
        "  def change_probabilities(self, new_probabilities):\n",
        "    self.probabilities = new_probabilities\n",
        "\n",
        "  def __get_gammas_and_deltas(self, formula, n_variable, vector):\n",
        "    #Check the first element\n",
        "    if isinstance(formula[1], list): gammas1, deltas1, variables1 = self.__get_gammas_and_deltas(formula[1], n_variable, vector)\n",
        "    else:\n",
        "      if (formula[1] == n_variable):\n",
        "        gammas1, deltas1 = [1], [0]\n",
        "        variables1 = set([formula[1]])\n",
        "      elif (formula[1] == -n_variable):\n",
        "        gammas1, deltas1 = [0], [1]\n",
        "        variables1 = set([abs(formula[1])])\n",
        "      elif (formula[1] == abs(formula[1])):\n",
        "        gammas1 = [self.probabilities[formula[1]-1], vector[formula[1]-1]]\n",
        "        deltas1 = [self.probabilities[formula[1]-1], vector[formula[1]-1]]\n",
        "        variables1 = set([formula[1]])\n",
        "      else:\n",
        "        gammas1 = [1-self.probabilities[abs(formula[1])-1], 1-vector[abs(formula[1])-1]]\n",
        "        deltas1 = [1-self.probabilities[abs(formula[1])-1], 1-vector[abs(formula[1])-1]]\n",
        "        variables1 = set([abs(formula[1])])\n",
        "    #Check the second element\n",
        "    if isinstance(formula[2], list): gammas2, deltas2, variables2 = self.__get_gammas_and_deltas(formula[2], n_variable, vector)\n",
        "    else:\n",
        "      if (formula[2] == n_variable):\n",
        "        gammas2, deltas2 = [1], [0]\n",
        "        variables2 = set([formula[2]])\n",
        "      elif (formula[2] == -n_variable):\n",
        "        gammas2, deltas2 = [0], [1]\n",
        "        variables2 = set([abs(formula[2])])\n",
        "      elif (formula[2] == abs(formula[2])):\n",
        "        gammas2 = [self.probabilities[formula[2]-1], vector[formula[2]-1]]\n",
        "        deltas2 = [self.probabilities[formula[2]-1], vector[formula[2]-1]]\n",
        "        variables2 = set([formula[2]])\n",
        "      else:\n",
        "        gammas2 = [1-self.probabilities[abs(formula[2])-1], 1-vector[abs(formula[2])-1]]\n",
        "        deltas2 = [1-self.probabilities[abs(formula[2])-1], 1-vector[abs(formula[2])-1]]\n",
        "        variables2 = set([abs(formula[2])])\n",
        "    gammas, deltas, variables = [], [], set()\n",
        "    #It's a disjunction\n",
        "    if (formula[0]):\n",
        "      for l in range(len(gammas1)):\n",
        "        gammas += [gammas1[l] + gammas2[l]]\n",
        "        deltas += [deltas1[l] + deltas2[l]]\n",
        "      variables = variables1.copy()\n",
        "    #It's a conjunction\n",
        "    else:\n",
        "      variables = variables1.union(variables2)\n",
        "      for l in range(len(variables) - (n_variable in variables) + 1):\n",
        "        gamma_temp = 0\n",
        "        delta_temp = 0\n",
        "        for l1 in range(len(gammas1)):\n",
        "          for l2 in range(len(gammas2)):\n",
        "            if (l1+l2 == l):\n",
        "              gamma_temp += gammas1[l1]*gammas2[l2]\n",
        "              delta_temp += deltas1[l1]*deltas2[l2]\n",
        "        gammas += [gamma_temp]\n",
        "        deltas += [delta_temp]\n",
        "    return gammas, deltas, variables\n",
        "\n",
        "  def obtain_SHAP(self, n_variable, vector):\n",
        "    if (len(vector) != len(self.probabilities)): raise InvalidInputs4SHAP()\n",
        "    if (self.formula == \"1\"): return 1/len(self.probabilities)\n",
        "    elif (self.formula == []): return -1/len(self.probabilities)\n",
        "    else:\n",
        "      gammas, deltas = self.__get_gammas_and_deltas(self.formula, n_variable, vector)[:2]\n",
        "      SHAP = 0\n",
        "      n_X = len(gammas)\n",
        "      for i in range(n_X):\n",
        "        SHAP += (factorial(i)*factorial(n_X-i-1)/factorial(n_X))*(vector[n_variable-1] - self.probabilities[n_variable-1])*(gammas[i] - deltas[i])\n",
        "      return SHAP\n",
        "\n",
        "  def obtain_SHAPs(self, vector):\n",
        "    SHAPs = []\n",
        "    for i in range(len(vector)):\n",
        "      SHAPs += [self.obtain_SHAP(i+1, vector)]\n",
        "    return SHAPs\n",
        "\n",
        "  def __form_graph(self, formula, level=0):\n",
        "    self.the_level = min(self.the_level, level)\n",
        "    if (formula[0]): #It's a disjunction\n",
        "      id_a = \"o\"+str(self.ind_o)\n",
        "      self.ind_o += 1\n",
        "      self.G.add_node(id_a, layer=level)\n",
        "      self.nodes_color += [\"hotpink\"]\n",
        "      self.labels[id_a] = r\"$\\vee$\"\n",
        "    else: #It's a conjunction\n",
        "      id_a = \"y\"+str(self.ind_a)\n",
        "      self.ind_a += 1\n",
        "      self.G.add_node(id_a, layer=level)\n",
        "      self.nodes_color += [\"c\"]\n",
        "      self.labels[id_a] = r\"$\\wedge$\"\n",
        "    for i in [1,2]:\n",
        "      if (isinstance(formula[i], list)):\n",
        "        id_b = self.__form_graph(formula[i], level-1)\n",
        "      else:\n",
        "        if (formula[i] == abs(formula[i])):\n",
        "          id_b = \"e\"+str(self.ind_E)\n",
        "          self.ind_E += 1\n",
        "          self.G.add_node(id_b, layer=level-1)\n",
        "          self.nodes_color += [\"moccasin\"]\n",
        "          self.labels[id_b] = fr\"$x_{formula[i]}$\"\n",
        "        else:\n",
        "          id_b = \"e\"+str(self.ind_E)\n",
        "          self.ind_E += 1\n",
        "          self.G.add_node(id_b, layer=level-1)\n",
        "          self.nodes_color += [\"limegreen\"]\n",
        "          self.labels[id_b] = fr\"$-x_{abs(formula[i])}$\"\n",
        "      self.G.add_edge(id_a, id_b)\n",
        "    return id_a\n",
        "\n",
        "  def __form_graph_alt(self, formula, level=0):\n",
        "    self.the_level = min(self.the_level, level)\n",
        "    if (formula[0]): #It's a disjunction\n",
        "      id_a = \"o\"+str(self.ind_o)\n",
        "      self.ind_o += 1\n",
        "      self.G.add_node(id_a, layer=level)\n",
        "      self.nodes_color += [\"hotpink\"]\n",
        "      self.labels[id_a] = r\"$\\vee$\"\n",
        "    else: #It's a conjunction\n",
        "      id_a = \"y\"+str(self.ind_a)\n",
        "      self.ind_a += 1\n",
        "      self.G.add_node(id_a, layer=level)\n",
        "      self.nodes_color += [\"c\"]\n",
        "      self.labels[id_a] = r\"$\\wedge$\"\n",
        "    for i in [1,2]:\n",
        "      if (isinstance(formula[i], list)):\n",
        "        id_b = self.__form_graph_alt(formula[i], level-1)\n",
        "      else:\n",
        "        if (formula[i] == abs(formula[i])):\n",
        "          id_b = \"e\"+str(self.ind_E)\n",
        "          self.ind_E += 1\n",
        "          self.G.add_node(id_b, layer=level-1)\n",
        "          self.nodes_color += [\"moccasin\"]\n",
        "          self.labels[id_b] = fr\"$x_{formula[i]}$\"\n",
        "        else:\n",
        "          id_b = \"n\"+str(self.ind_n)\n",
        "          id_c = \"e\"+str(self.ind_E)\n",
        "          self.ind_n += 1\n",
        "          self.ind_E += 1\n",
        "          self.G.add_node(id_b, layer=level-1)\n",
        "          self.G.add_node(id_c, layer=level-2)\n",
        "          self.nodes_color += [\"limegreen\"]\n",
        "          self.nodes_color += [\"moccasin\"]\n",
        "          self.labels[id_b] = fr\"$-$\"\n",
        "          self.labels[id_c] = fr\"$x_{abs(formula[i])}$\"\n",
        "          self.G.add_edge(id_c, id_b)\n",
        "      self.G.add_edge(id_b, id_a)\n",
        "    return id_a\n",
        "\n",
        "  def plot_formula(self, resplt=10, alternative=False):\n",
        "    self.labels = {}\n",
        "    self.nodes_color = []\n",
        "    self.ind_a = 1\n",
        "    self.ind_o = 1\n",
        "    self.ind_T = 1\n",
        "    self.ind_E = 1\n",
        "    self.ind_n = 1\n",
        "    self.the_level = 0\n",
        "    self.G = nx.DiGraph()\n",
        "    if (self.formula == \"1\"):\n",
        "      self.G.add_node(\"T\", layer=1)\n",
        "      self.nodes_color += [\"coral\"]\n",
        "      self.labels[\"T\"] = fr\"$T$\"\n",
        "    elif (self.formula == []):\n",
        "      self.G.add_node(\"F\", layer=1)\n",
        "      self.nodes_color += [\"thistle\"]\n",
        "      self.labels[\"F\"] = fr\"$F$\"\n",
        "    elif (len(self.formula) == 1):\n",
        "      if (self.formula[0] == abs(self.formula[0])):\n",
        "        self.G.add_node(\"e\", layer=1)\n",
        "        self.nodes_color += [\"moccasin\"]\n",
        "        self.labels[\"e\"] = fr\"$x_{self.formula[0]}$\"\n",
        "      else:\n",
        "        self.G.add_node(\"e\", layer=1)\n",
        "        self.nodes_color += [\"limegreen\"]\n",
        "        self.labels[\"e\"] = fr\"$-x_{abs(self.formula[0])}$\"\n",
        "    elif (alternative): ignorable = self.__form_graph_alt(self.formula)\n",
        "    else: ignorable = self.__form_graph(self.formula)\n",
        "    if (self.the_level == 0): self.the_level = 1\n",
        "    pos = nx.multipartite_layout(self.G, subset_key=\"layer\", align=\"horizontal\")\n",
        "    plt.clf()\n",
        "    plt.figure(figsize=[resplt,resplt])\n",
        "    nx.draw_networkx(self.G, pos, node_size=200*resplt/abs(self.the_level), with_labels=True, node_color=self.nodes_color,\n",
        "                     labels=self.labels, font_size=16*resplt/(abs(self.the_level)+resplt), arrows=alternative, width=(1+alternative))\n",
        "    plt.show()\n",
        "    del self.labels, self.nodes_color, self.ind_a, self.ind_o, self.ind_T, self.ind_E, self.the_level, self.G\n",
        "\n",
        "  def corroborate_equivalence(self, the_model, test=-1):\n",
        "    equivalent = True\n",
        "    combinations = np.array(list(itertools.product([-1, 1], repeat=self.n_vars)))\n",
        "    if (test < 0): relevants = len(combinations)\n",
        "    else: relevants = test\n",
        "    for i in combinations[:relevants]:\n",
        "      res_net = the_model.predict(i.reshape(1,-1), verbose=0)[0][0]\n",
        "      res_cir = self.evaluate_formula(i)\n",
        "      if (res_net != res_cir):\n",
        "        equivalent = False\n",
        "        break\n",
        "    if (equivalent): print(f'They are equivalent for the {min(relevants, len(combinations))} values reviewed')\n",
        "    else: print(\"They aren't equivalent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dKBZFRGN0Oc"
      },
      "source": [
        "## SHAP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Based on \"shap-score-script.py\" of Maximilian Schleich\n",
        "import pandas as pd\n",
        "from multiprocessing import Pool\n",
        "\n",
        "#Method to compute all required factorials\n",
        "def factorial_list(N):\n",
        "  fact = [0] * (N + 1)\n",
        "  fact[0] = 1\n",
        "  fact[1] = 1\n",
        "  for num in range(2, N + 1): fact[num] = fact[num-1] * num\n",
        "  return fact\n",
        "\n",
        "def generate_subsets(n_vairables, protected_variable, subsets=[[]], actual_variable=0):\n",
        "  if (n_vairables == actual_variable): return subsets\n",
        "  elif (protected_variable==actual_variable): return generate_subsets(n_vairables, protected_variable, subsets, actual_variable+1)\n",
        "  else: return generate_subsets(n_vairables, protected_variable, [i+[actual_variable] for i in subsets]+subsets, actual_variable+1)\n",
        "\n",
        "def SHAP_values(all_combinations, entity, counter, total):\n",
        "  n_variables = all_combinations.shape[1]-1\n",
        "  factorials = factorial_list(n_variables)\n",
        "  cols = all_combinations.columns\n",
        "  SHAPs = []\n",
        "  for variable in range(n_variables):\n",
        "    SHAP = 0\n",
        "    subsets_variables = generate_subsets(n_variables, variable)\n",
        "    for subset in subsets_variables:\n",
        "      if (len(subset) > 0):\n",
        "        cond = f'`{cols[subset[0]]}` == {entity[subset[0]]}'\n",
        "        if (len(subset) > 1):\n",
        "          for x in subset[1:]: cond += f' & `{cols[x]}` == {entity[x]}'\n",
        "        rows_without_variable = all_combinations.query(cond)\n",
        "      else: rows_without_variable = all_combinations\n",
        "      aux = rows_without_variable.query(f'`{cols[variable]}` == {entity[variable]}')['prediction'].sum()/(2**(n_variables-1-len(subset)))\n",
        "      aux -= rows_without_variable['prediction'].sum()/(2**(n_variables-len(subset)))\n",
        "      SHAP += (factorials[len(subset)]*factorials[n_variables-len(subset)-1]/factorials[n_variables])*aux\n",
        "    SHAPs += [SHAP]\n",
        "  if (counter%4 == 1): print(counter, \"of\", total, \"|\", time.asctime(time.localtime(time.time())))\n",
        "  return SHAPs\n",
        "\n",
        "def SHAP_matrix_bnn(model, df, limit_d=None, limit_u=None): \n",
        "  shap_mat = [] \n",
        "  res = []\n",
        "  full_df = pd.DataFrame(np.array(list(itertools.product([-1, 1], repeat=model.input_shape[1]))), columns=df.columns)\n",
        "  full_df['prediction'] = model.predict(full_df)[:,0]\n",
        "  clean_df = df.drop_duplicates().reset_index(drop=True)\n",
        "  if (limit_u != None): clean_df = clean_df.head(limit_u)\n",
        "  if (limit_d != None): clean_df = clean_df.tail(-limit_d)\n",
        "  with Pool(processes=1, maxtasksperchild=100) as pool:\n",
        "    for index, entity in clean_df.iterrows():\n",
        "      res.append(pool.apply_async(SHAP_values, (full_df, entity, index+1, limit_u)))\n",
        "    for r in res: \n",
        "      shap_mat.append(r.get())\n",
        "  return pd.DataFrame(shap_mat, columns=df.columns)\n",
        "\n",
        "def SHAP_matrix_bc(circuit, df, limit_d=None, limit_u=None):\n",
        "  shap_mat = [] \n",
        "  res = []\n",
        "  full_df = pd.DataFrame(np.array(list(itertools.product([-1, 1], repeat=circuit.n_vars))), columns=df.columns)\n",
        "  full_df['prediction'] = circuit.evaluate_formulas(full_df)\n",
        "  clean_df = df.drop_duplicates().reset_index(drop=True)\n",
        "  if (limit_u != None): clean_df = clean_df.head(limit_u)\n",
        "  if (limit_d != None): clean_df = clean_df.tail(-limit_d)\n",
        "  with Pool(processes=1, maxtasksperchild=100) as pool:\n",
        "    for index, entity in clean_df.iterrows():\n",
        "      res.append(pool.apply_async(SHAP_values, (full_df, entity, index+1, limit_u)))\n",
        "    for r in res: \n",
        "      shap_mat.append(r.get())\n",
        "  return pd.DataFrame(shap_mat, columns=df.columns)\n",
        "\n",
        "def SHAP_matrix_bc_fast(circuit, df, limit_d=None, limit_u=None):\n",
        "  SHAPs = []\n",
        "  counter = 1\n",
        "  clean_df = df.drop_duplicates()\n",
        "  if (limit_u != None): clean_df = clean_df.head(limit_u)\n",
        "  if (limit_d != None): clean_df = clean_df.tail(-limit_d)\n",
        "  for observation in (clean_df.to_numpy()+1)/2:\n",
        "    if (counter%10 == 1): print(counter, \"of\", len(clean_df), \"|\", time.asctime(time.localtime(time.time())))\n",
        "    counter += 1\n",
        "    SHAPs += [circuit.obtain_SHAPs(observation)]\n",
        "  return pd.DataFrame(SHAPs, columns=df.columns)"
      ],
      "metadata": {
        "id": "Fs5cLbSZ_kha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing of the dataset"
      ],
      "metadata": {
        "id": "GUItxykBr5gn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuHgc1IMCxtY"
      },
      "source": [
        "Data source: https://www.kaggle.com/datasets/camnugent/california-housing-prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN_F2iSHCxtZ"
      },
      "outputs": [],
      "source": [
        "print(\"Raw data\")\n",
        "dataset_url = \"https://raw.githubusercontent.com/Jorvan758/dDBCSFi2/main/housingc.csv\"\n",
        "pd.read_csv(dataset_url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([pd.read_csv(dataset_url).min(axis=0), pd.read_csv(dataset_url).max(axis=0)], axis=1, keys=[\"min\", \"max\"])"
      ],
      "metadata": {
        "id": "vye17DmmENGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(dataset_url)[\"ocean_proximity\"].drop_duplicates()"
      ],
      "metadata": {
        "id": "LoK5bis7EfzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0wk07M7Cxti"
      },
      "outputs": [],
      "source": [
        "print(\"Preprocessed data\")\n",
        "df = pd.read_csv(dataset_url)\n",
        "df = (df > df.mean(axis=0))\n",
        "df = pd.concat([pd.get_dummies(pd.read_csv(\"housingc.csv\")[\"ocean_proximity\"], prefix=\"op\").astype(int),\n",
        "                df.drop(\"ocean_proximity\", axis=1).astype(int)], axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFYE4TlGCxti"
      },
      "outputs": [],
      "source": [
        "pearson_corr = df.corr(method='pearson').round(3)\n",
        "plt.tight_layout()\n",
        "fig, ax = plt.subplots(figsize=(len(df.columns)-1,len(df.columns)-1))\n",
        "ax = sns.heatmap(pearson_corr,cmap='magma',linecolor='black',linewidths=1,annot=True)\n",
        "ax.set_title('Pearson correlation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1828LPWlCxtj"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(\"median_house_value\", axis=1), df[\"median_house_value\"],\n",
        "                                                    test_size=0.5, stratify=df[\"median_house_value\"], random_state=7)\n",
        "X_train = X_train*2-1\n",
        "X_test = X_test*2-1\n",
        "X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "o0snw6oA1BMn"
      },
      "source": [
        "## BNN training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZEpbbGr1BMs"
      },
      "outputs": [],
      "source": [
        "set_random_seeds()\n",
        "\n",
        "n_input = len(X_train.columns)\n",
        "n_neurons = n_input\n",
        "input_layer = layers.Input(shape=(n_input,), name=\"Input_layer\")\n",
        "dense_layer1 = lq.layers.QuantDense(n_neurons, kernel_quantizer=\"ste_sign\", kernel_constraint=\"weight_clip\",\n",
        "                                    kernel_initializer=RandomNormal(stddev=0.1),\n",
        "                                    bias_initializer=RandomNormal(stddev=0.1),\n",
        "                                    use_bias=True, name=\"Dense_layer_1\")(input_layer)\n",
        "output_layer = lq.layers.QuantDense(1, input_quantizer=\"ste_sign\", kernel_quantizer=\"ste_sign\",\n",
        "                                    kernel_constraint=\"weight_clip\", activation=\"sigmoid\",\n",
        "                                    kernel_initializer=RandomNormal(stddev=0.1),\n",
        "                                    bias_initializer=RandomNormal(stddev=0.1),\n",
        "                                    use_bias=True, name=\"Output_layer\")(dense_layer1)\n",
        "The_model = models.Model(input_layer, output_layer, name=\"The_model\")\n",
        "The_model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "The_model.summary()\n",
        "print()\n",
        "\n",
        "n_epochs = 200\n",
        "callback = callbacks.EarlyStopping(monitor='loss', patience=n_epochs//5, restore_best_weights=True)\n",
        "beginning = time.monotonic()\n",
        "record = The_model.fit(X_train, y_train, batch_size=len(X_train), epochs=n_epochs, verbose=1,\n",
        "                          validation_data=(X_test, y_test), shuffle=True, callbacks=[callback])\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"\\nTime taken to train:\", seconds_separator(duration),\"\\n\")\n",
        "\n",
        "fig, axs= plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
        "plt.sca(axs[0])\n",
        "plt.plot(record.epoch[1:]+[len(record.epoch)], record.history[\"loss\"], label = \"Training data\")\n",
        "plt.plot(record.epoch[1:]+[len(record.epoch)], record.history[\"val_loss\"], label = \"Test data\")\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.sca(axs[1])\n",
        "plt.plot(record.epoch[1:]+[len(record.epoch)], record.history[\"accuracy\"], label = \"Training data\")\n",
        "plt.plot(record.epoch[1:]+[len(record.epoch)], record.history[\"val_accuracy\"], label = \"Test data\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOSS-hZM1BMt"
      },
      "outputs": [],
      "source": [
        "print(\"Before binarizing (Loss & Accuracy)\")\n",
        "print(\"Training:\", The_model.evaluate(X_train, y_train, verbose=0))\n",
        "print(\"Testing:\", The_model.evaluate(X_test, y_test, verbose=0))\n",
        "The_model.layers[2].activation = step_sigm\n",
        "The_model.set_weights([((i>0)*2-1)*(i.ndim==2)+(i)*(i.ndim==1) for i in The_model.get_weights()])\n",
        "print(\"\\nAfter binarizing (Loss & Accuracy)\")\n",
        "print(\"Training:\", The_model.evaluate(X_train, y_train, verbose=0))\n",
        "print(\"Testing:\", The_model.evaluate(X_test, y_test, verbose=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "E9ffQnPh1BMu"
      },
      "source": [
        "## Conversion of the BNN to a dDBCSFi(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cujPBnrD1BMu"
      },
      "outputs": [],
      "source": [
        "beginning = time.monotonic()\n",
        "cnff_name = 'CNFf.cnf'\n",
        "cnff_name = encode_network(The_model, cnff_name)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"Time taken to create the formula:\", seconds_separator(duration),\"\\n\")\n",
        "\n",
        "beginning = time.monotonic()\n",
        "mgr = SddManager()\n",
        "ssd_manager, node = mgr.from_cnf_file(bytes(cnff_name, encoding='utf-8'))\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"Time taken to create the SDD:\", seconds_separator(duration),\"\\n\")\n",
        "\n",
        "beginning = time.monotonic()\n",
        "The_circuit = dDBCSFi_2(n_input, SDD=node)\n",
        "The_circuit.compile_bnn()\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"Time taken to create the dDBCSFi(2):\", seconds_separator(duration))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I56Ozaig1BMv"
      },
      "outputs": [],
      "source": [
        "beginning = time.monotonic()\n",
        "The_circuit.corroborate_equivalence(The_model, -1)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"Time taken to verify the equivalence of dDBCSFi(2) with the BNN:\", seconds_separator(duration))\n",
        "\n",
        "print(\"\\nThe circuit has\", The_circuit.count_nodes(), \"nodes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwaLmSv51BMv"
      },
      "outputs": [],
      "source": [
        "plot_the_network(The_model, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEe-GW6X1BMw"
      },
      "outputs": [],
      "source": [
        "The_circuit.plot_formula(60)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "781V3d3-zaQ_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbOedVt9RgBc"
      },
      "source": [
        "## SHAP on the BNN, black-box"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beginning = time.monotonic()\n",
        "SHAPs_1 = SHAP_matrix_bnn(The_model, X_train, None, 20)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"\\nTime taken to calculate the SHAPs for the original model:\", seconds_separator(duration))\n",
        "\n",
        "SHAPs_1 = pd.DataFrame(SHAPs_1, columns=X_train.columns)\n",
        "SHAPs_1.to_csv(\"BNN_SHAPs_direct_1-20.csv\")\n",
        "SHAPs_1"
      ],
      "metadata": {
        "id": "ciagIVUXcZ91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beginning = time.monotonic()\n",
        "SHAPs_1 = SHAP_matrix_bnn(The_model, X_train, 20, 40)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"\\nTime taken to calculate the SHAPs for the original model:\", seconds_separator(duration))\n",
        "\n",
        "SHAPs_1 = pd.DataFrame(SHAPs_1, columns=X_train.columns)\n",
        "SHAPs_1.to_csv(\"BNN_SHAPs_direct_21-40.csv\")\n",
        "SHAPs_1"
      ],
      "metadata": {
        "id": "y5r50d4QcuLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beginning = time.monotonic()\n",
        "SHAPs_1 = SHAP_matrix_bnn(The_model, X_train, 40, 60)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"\\nTime taken to calculate the SHAPs for the original model:\", seconds_separator(duration))\n",
        "\n",
        "SHAPs_1 = pd.DataFrame(SHAPs_1, columns=X_train.columns)\n",
        "SHAPs_1.to_csv(\"BNN_SHAPs_direct_41-60.csv\")\n",
        "SHAPs_1"
      ],
      "metadata": {
        "id": "0uOKXfKDc5cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beginning = time.monotonic()\n",
        "SHAPs_1 = SHAP_matrix_bnn(The_model, X_train, 60, 80)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"\\nTime taken to calculate the SHAPs for the original model:\", seconds_separator(duration))\n",
        "\n",
        "SHAPs_1 = pd.DataFrame(SHAPs_1, columns=X_train.columns)\n",
        "SHAPs_1.to_csv(\"BNN_SHAPs_direct_61-80.csv\")\n",
        "SHAPs_1"
      ],
      "metadata": {
        "id": "3_8jRQchc9SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beginning = time.monotonic()\n",
        "SHAPs_1 = SHAP_matrix_bnn(The_model, X_train, 80, 100)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"\\nTime taken to calculate the SHAPs for the original model:\", seconds_separator(duration))\n",
        "\n",
        "SHAPs_1 = pd.DataFrame(SHAPs_1, columns=X_train.columns)\n",
        "SHAPs_1.to_csv(\"BNN_SHAPs_direct_81-100.csv\")\n",
        "SHAPs_1"
      ],
      "metadata": {
        "id": "4853kVXmc_J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHAPs_1 = pd.concat([pd.read_csv('BNN_SHAPs_direct_1-20.csv').iloc[:,1:],\n",
        "                     pd.read_csv('BNN_SHAPs_direct_21-40.csv').iloc[:,1:],\n",
        "                     pd.read_csv('BNN_SHAPs_direct_41-60.csv').iloc[:,1:],\n",
        "                     pd.read_csv('BNN_SHAPs_direct_61-80.csv').iloc[:,1:],\n",
        "                     pd.read_csv('BNN_SHAPs_direct_81-100.csv').iloc[:,1:]]).reset_index(drop=True)\n",
        "print(pd.concat([SHAPs_1.mean(axis=0).round(6), SHAPs_1.std(axis=0).round(6)], axis=1), \"\\n\\n\",\n",
        "      pd.concat([SHAPs_1.abs().mean(axis=0).round(6), SHAPs_1.abs().std(axis=0).round(6)], axis=1), sep='')"
      ],
      "metadata": {
        "id": "yRGICwGBRgBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP8-TzxfmRlv"
      },
      "source": [
        "## SHAP on the dDBCSFi(2), black-box"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beginning = time.monotonic()\n",
        "SHAPs_2 = SHAP_matrix_bc(The_circuit, X_train, None, 20)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"\\nTime taken to calculate the SHAPs for the circuit, without the algorithm:\", seconds_separator(duration))\n",
        "\n",
        "SHAPs_2 = pd.DataFrame(SHAPs_2, columns=X_train.columns)\n",
        "SHAPs_1.to_csv(\"dDBCSFi(2)_SHAPs_direct_1-20.csv\")\n",
        "SHAPs_2"
      ],
      "metadata": {
        "id": "9MV4xsDRmRlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beginning = time.monotonic()\n",
        "SHAPs_2 = SHAP_matrix_bc(The_circuit, X_train, 20, 40)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"\\nTime taken to calculate the SHAPs for the circuit, without the algorithm:\", seconds_separator(duration))\n",
        "\n",
        "SHAPs_2 = pd.DataFrame(SHAPs_2, columns=X_train.columns)\n",
        "SHAPs_1.to_csv(\"dDBCSFi(2)_SHAPs_direct_21-40.csv\")\n",
        "SHAPs_2"
      ],
      "metadata": {
        "id": "WcV_ecA5mRlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beginning = time.monotonic()\n",
        "SHAPs_2 = SHAP_matrix_bc(The_circuit, X_train, 40, 60)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"\\nTime taken to calculate the SHAPs for the circuit, without the algorithm:\", seconds_separator(duration))\n",
        "\n",
        "SHAPs_2 = pd.DataFrame(SHAPs_2, columns=X_train.columns)\n",
        "SHAPs_1.to_csv(\"dDBCSFi(2)_SHAPs_direct_41-60.csv\")\n",
        "SHAPs_2"
      ],
      "metadata": {
        "id": "R7CKBq5jmRlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beginning = time.monotonic()\n",
        "SHAPs_2 = SHAP_matrix_bc(The_circuit, X_train, 60, 80)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"\\nTime taken to calculate the SHAPs for the circuit, without the algorithm:\", seconds_separator(duration))\n",
        "\n",
        "SHAPs_2 = pd.DataFrame(SHAPs_2, columns=X_train.columns)\n",
        "SHAPs_1.to_csv(\"dDBCSFi(2)_SHAPs_direct_61-80.csv\")\n",
        "SHAPs_2"
      ],
      "metadata": {
        "id": "6ZmoXvtAmRlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beginning = time.monotonic()\n",
        "SHAPs_2 = SHAP_matrix_bc(The_circuit, X_train, 80, 100)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"\\nTime taken to calculate the SHAPs for the circuit, without the algorithm:\", seconds_separator(duration))\n",
        "\n",
        "SHAPs_2 = pd.DataFrame(SHAPs_2, columns=X_train.columns)\n",
        "SHAPs_1.to_csv(\"dDBCSFi(2)_SHAPs_direct_81-100.csv\")\n",
        "SHAPs_2"
      ],
      "metadata": {
        "id": "duUge_VmmRlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHAPs_2 = pd.concat([pd.read_csv('dDBCSFi(2)_SHAPs_direct_1-20.csv').iloc[:,1:],\n",
        "                     pd.read_csv('dDBCSFi(2)_SHAPs_direct_21-40.csv').iloc[:,1:],\n",
        "                     pd.read_csv('dDBCSFi(2)_SHAPs_direct_41-60.csv').iloc[:,1:],\n",
        "                     pd.read_csv('dDBCSFi(2)_SHAPs_direct_61-80.csv').iloc[:,1:],\n",
        "                     pd.read_csv('dDBCSFi(2)_SHAPs_direct_81-100.csv').iloc[:,1:]]).reset_index(drop=True)\n",
        "print(pd.concat([SHAPs_1.mean(axis=0).round(6), SHAPs_1.std(axis=0).round(6)], axis=1), \"\\n\\n\",\n",
        "      pd.concat([SHAPs_1.abs().mean(axis=0).round(6), SHAPs_1.abs().std(axis=0).round(6)], axis=1), sep='')"
      ],
      "metadata": {
        "id": "zSVTvU3BCnXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUSo2kVnlNjj"
      },
      "source": [
        "## SHAP on the dDBCSFi(2), open-box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOnyolGDlNjl"
      },
      "outputs": [],
      "source": [
        "beginning = time.monotonic()\n",
        "SHAPs_3 = SHAP_matrix_bc_fast(The_circuit, X_train, None, 100)\n",
        "duration = time.monotonic() - beginning\n",
        "print(\"\\nTime taken to calculate the SHAPs for the circuit, with the algorithm:\", seconds_separator(duration))\n",
        "\n",
        "SHAPs_3 = pd.DataFrame(SHAPs_3, columns=X_train.columns)\n",
        "SHAPs_3.to_csv(\"dDBCSFi(2)_SHAPs_efficient.csv\")\n",
        "SHAPs_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4fR9L4QlNjm"
      },
      "outputs": [],
      "source": [
        "print(pd.concat([SHAPs_3.mean(axis=0).round(6), SHAPs_3.std(axis=0).round(6)], axis=1), \"\\n\\n\",\n",
        "      pd.concat([SHAPs_3.abs().mean(axis=0).round(6), SHAPs_3.abs().std(axis=0).round(6)], axis=1), sep='')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "75X9dbSdDI_s",
        "9e58yPuk2k20",
        "VtjZE5zd3e3H",
        "rK9TuJLY3YQk",
        "IuYCpLzd3Z5v",
        "W2zuG8d-6yYT",
        "g370GikG3buz",
        "udShaH-e3ong",
        "6dKBZFRGN0Oc",
        "GUItxykBr5gn",
        "o0snw6oA1BMn",
        "E9ffQnPh1BMu",
        "781V3d3-zaQ_",
        "bbOedVt9RgBc",
        "UP8-TzxfmRlv",
        "TUSo2kVnlNjj"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
